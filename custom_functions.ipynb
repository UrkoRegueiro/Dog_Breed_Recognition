{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a834ed43-17e7-41a1-9a1e-6f00e8660a43",
   "metadata": {},
   "source": [
    "# En este notebook se encuentras las funciones utilizadas a lo largo del proyecto\n",
    "<span style=\"font-size:Large;\">       \n",
    "Se ha instalado el siguiente paquete para cargar las funciones entre notebooks:<br>    \n",
    "<br> \n",
    "    \n",
    "```python\n",
    "! pip install nbimporter\n",
    "```\n",
    "<br>     \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c5941c-d528-4ff6-a2b6-e732412cdf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\regue\\conda_ENV\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Básicos:\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from joblib import dump, load\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# CNN:\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Métricas:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Pretrained CNN:\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "# Time:\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7a620-21ae-4ec6-a7e8-d6caeaf76179",
   "metadata": {},
   "source": [
    "# Funciones procesamiento de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1e5df4-23f1-41b1-beeb-9694006bae30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_processor(images, high, width):\n",
    "    images_processed = []\n",
    "\n",
    "    for image in images:\n",
    "        image_processed = image / 255\n",
    "        image_resized = cv2.resize(src = image_processed, dsize = (high, width))\n",
    "        images_processed.append(image_resized)\n",
    "\n",
    "    return images_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aab1a32-f03b-4d13-8d81-01aa6d0636c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_data(X_train, y_train):\n",
    "\n",
    "    data_augmentation = tf.keras.Sequential([layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                                             layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "                                             layers.experimental.preprocessing.RandomZoom(0.1)])\n",
    "\n",
    "    X_train_aug = []\n",
    "    y_train_aug = []\n",
    "\n",
    "    for image, breed in tqdm.tqdm(zip(X_train, y_train)):\n",
    "\n",
    "        img = tf.expand_dims(image, 0)\n",
    "        for i in range(9):\n",
    "\n",
    "            augmented_image = data_augmentation(img)\n",
    "            X_train_aug.append(augmented_image[0])\n",
    "            y_train_aug.append(breed)\n",
    "            \n",
    "    return X_train_aug, y_train_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9998caf-235b-4841-b4cb-90de6d5cc715",
   "metadata": {},
   "source": [
    "# Funciones Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ece1b-3ea4-4bcc-b763-90e459a49fe2",
   "metadata": {},
   "source": [
    "- ## Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79fbc4d-2a41-4e8d-91c2-42a4c875822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cnn(X_train, y_train, X_val, y_val, epochs, batch, call_backs= True):\n",
    "    \n",
    "    num_classes = len(y_train[0])\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Feature training:\n",
    "        # Capa de entrada:\n",
    "    model.add(Conv2D(filters= 16, kernel_size= 2, padding= 'same', activation= 'relu', input_shape= (224, 224, 3)))\n",
    "    model.add(MaxPooling2D(pool_size= 2))\n",
    "\n",
    "        # Conv_pooling_1\n",
    "    model.add(Conv2D(filters= 32, kernel_size= 2 , padding= 'same' , activation= 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size= 2))\n",
    "\n",
    "        # Conv_pooling_2\n",
    "    model.add(Conv2D(filters= 64 , kernel_size= 2 , padding= 'same' , activation= 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size= 2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "        # Conv_pooling_3\n",
    "    model.add(Conv2D(filters= 128 , kernel_size= 2 , padding= 'same' , activation= 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size= 2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully conecting\n",
    "    model.add(Dense(512, activation= 'relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Salida:\n",
    "    model.add(Dense(num_classes, activation= 'softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss= \"categorical_crossentropy\", optimizer= \"adam\", metrics= [\"accuracy\"])\n",
    "    \n",
    "    if call_backs:\n",
    "        # Callbacks:\n",
    "        checkpoint = ModelCheckpoint('best_model_5_1.h5', monitor= 'val_loss', verbose= 0, save_best_only= True)\n",
    "        early_stops = EarlyStopping(patience= 4, monitor= 'val_loss')\n",
    "\n",
    "        # Entrenamos el modelo:\n",
    "        history = model.fit(x= X_train, y= y_train,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            batch_size= batch,\n",
    "                            epochs= epochs,\n",
    "                            callbacks= [checkpoint, early_stops]\n",
    "                           )\n",
    "    else:\n",
    "        history = model.fit(x= X_train, y= y_train,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            batch_size= batch,\n",
    "                            epochs= epochs)\n",
    "        \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd94b3c-7835-4564-9710-b91e1c1bfd62",
   "metadata": {},
   "source": [
    "- ## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b6dade-1c07-42c8-bf49-755a67bd8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xception_cnn(X_train, y_train, X_val, y_val, epochs, batch, call_backs= True):\n",
    "    \n",
    "    num_classes = len(y_train[0])\n",
    "    shape = X_train.shape[1:]\n",
    "    \n",
    "    xception_model = Xception(weights = \"imagenet\", include_top = False, input_shape = shape)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(xception_model)\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Añado una fully conecting layer para crear nuevas conexiones, ajustandose a nuestros datos:\n",
    "    model.add(Dense(512, activation= 'relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Salida:\n",
    "    model.add(Dense(num_classes, activation= 'softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss= \"categorical_crossentropy\", optimizer= \"adam\", metrics= [\"accuracy\"])\n",
    "    \n",
    "    if call_backs:\n",
    "        # Callbacks:\n",
    "        checkpoint = ModelCheckpoint('best_model_5_2.h5', monitor= 'val_loss', verbose= 0, save_best_only= True)\n",
    "        early_stops = EarlyStopping(patience= 4, monitor= 'val_loss')\n",
    "\n",
    "        # Entrenamos el modelo:\n",
    "        history = model.fit(x= X_train, y = y_train,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            batch_size= batch,\n",
    "                            epochs= epochs,\n",
    "                            callbacks= [checkpoint, early_stops]\n",
    "                           )\n",
    "    else:\n",
    "        history = model.fit(x= X_train, y= y_train,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            batch_size= batch,\n",
    "                            epochs= epochs)\n",
    "    \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c1b44-debd-4a90-970e-150a33471577",
   "metadata": {},
   "source": [
    "# Visualización resultados Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4507c2d-4390-4683-ba66-3150db035153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    accuracy     = history.history[\"accuracy\"]\n",
    "    loss         = history.history[\"loss\"]\n",
    "\n",
    "    val_accuracy = history.history[\"val_accuracy\"]\n",
    "    val_loss     = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(accuracy) + 1)\n",
    "    \n",
    "    # Plots:    \n",
    "    figure, axes = plt.subplots(1, 2, figsize = (12, 6))\n",
    "    axes = axes.flatten() \n",
    "\n",
    "    # Plot Accuracy\n",
    "    axes[0].plot(epochs, accuracy, \"r--\", label=\"Train accuracy\")\n",
    "    axes[0].plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "\n",
    "    axes[0].set_title(\"Training and validation accuracy\")\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot Loss\n",
    "    axes[1].plot(epochs, loss, \"r--\", label=\"Train loss\")\n",
    "    axes[1].plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "\n",
    "    axes[1].set_title(\"Training and validation loss\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
